from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.prompts import ChatPromptTemplate
import streamlit as st

from utils.chat_utils import initialize_messages, send_message, load_chat_history
from utils.callback_utils import ChatCallbackHandler
from utils.document_utils import (
    format_documents,
    create_text_splitter,
    save_uploaded_file,
    create_cache_dir,
)
from utils.retriever_utils import create_ensemble_retriever

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
        ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
        1. ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
        2. ì œê³µëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        3. ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”

        ë¬¸ì„œ ë‚´ìš©:
        {context}
        """,
        ),
        ("human", "{question}"),
    ]
)

llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.2,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)

initialize_messages()


@st.cache_resource(show_spinner="íŒŒì¼ì„ ë¶„ì„í•˜ê³ ìˆì–´ìš”...")
def embed_file(file):
    file_path = save_uploaded_file(file, "files")
    cache_dir = create_cache_dir(file.name, "embeddings")

    splitter = create_text_splitter()
    file_loader = UnstructuredFileLoader(file_path)
    docs = file_loader.load_and_split(text_splitter=splitter)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
    )

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    vector_retriever = vectorstore.as_retriever(
        search_type="mmr",  # MMR(Maximum Marginal Relevance) : ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ê³ ë ¤
        search_kwargs={
            "k": 4,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            "fetch_k": 20,  # í›„ë³´ í’€ í¬ê¸°
            "lambda_mult": 0.7,  # ë‹¤ì–‘ì„± vs ê´€ë ¨ì„± ê°€ì¤‘ì¹˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ì„± ì¤‘ì‹œ)
        },
    )

    return create_ensemble_retriever(vector_retriever, docs)


st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“„",
)

st.title("DocumentGPT")

st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”!

DocumentGPTëŠ” ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!

ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!
"""
)

st.sidebar.markdown(
    """
### ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
- PDF (.pdf)
- í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)
- Word ë¬¸ì„œ (.docx)

"""
)

with st.sidebar:
    file = st.file_uploader(
        ".txt, .pdf, .docx íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type=["txt", "pdf", "docx"]
    )

if file:
    retriever = embed_file(file)
    send_message("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!", "ai", save=False)
    load_chat_history()
    message = st.chat_input("ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")

    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_documents),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)

else:
    st.session_state["messages"] = []
