import streamlit as st
import subprocess
import math
from pydub import AudioSegment
import glob
import openai
import os
import yt_dlp
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import TextLoader
from langchain.schema import StrOutputParser
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.prompts import MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain.storage import LocalFileStore
from operator import itemgetter


from utils.document_utils import create_text_splitter
from utils.chat_utils import (
    initialize_messages,
    send_message,
    load_chat_history,
    save_message,
    save_memory,
)
from utils.callback_utils import ChatCallbackHandler

llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.1,
)

chat_llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

initialize_messages()
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        llm=llm, max_token_limit=1000, return_messages=True
    )
if "is_summary" not in st.session_state:
    st.session_state["is_summary"] = False


@st.cache_data()
def extract_audio(video_path):
    audio_path = video_path.replace("mp4", "mp3")
    command = ["ffmpeg", "-y", "-i", video_path, "-vn", audio_path]
    subprocess.run(command)


@st.cache_data()
def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):
    os.makedirs(chunks_folder, exist_ok=True)

    track = AudioSegment.from_mp3(audio_path)
    overlap_size = 10 * 1000
    chunk_len = chunk_size * 60 * 1000 - overlap_size
    chunks = math.ceil(len(track) / chunk_len)
    for i in range(chunks):
        start_time = i * chunk_len
        end_time = (i + 1) * chunk_len + overlap_size

        chunk = track[start_time:end_time]

        chunk.export(f"{chunks_folder}/chunk_{i:02d}.mp3", format="mp3")


@st.cache_data()
def transcript_chunks(chunks_folder, destination):
    chunks = glob.glob(f"{chunks_folder}/*.mp3")
    chunks.sort()
    for chunk in chunks:
        with open(chunk, "rb") as audio_file, open(destination, "a") as text_file:
            transcript = openai.audio.transcriptions.create(
                model="whisper-1", file=audio_file, language="en"
            )
            text_file.write(transcript.text)


@st.cache_data()
def download_audio_from_youtube(youtube_url, output_path):
    if os.path.exists(output_path):
        os.remove(output_path)

    ydl_opts = {
        "format": "bestaudio/best",
        "postprocessors": [
            {
                "key": "FFmpegExtractAudio",
                "preferredcodec": "mp3",
                "preferredquality": "192",
            }
        ],
        "outtmpl": output_path,
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([youtube_url])


@st.cache_data()
def summary_transcript(transcript_path):
    loader = TextLoader(transcript_path)
    splitter = create_text_splitter(chunk_size=800, chunk_overlap=100)
    docs = loader.load_and_split(text_splitter=splitter)

    first_summary_prompt = ChatPromptTemplate.from_template(
        """
            Write a concise summary of the following:
            "{text}"
            CONCISE SUMMARY:  
    """
    )

    first_summary_chain = first_summary_prompt | llm | StrOutputParser()

    summary = first_summary_chain.invoke({"text": docs[0].page_content})

    refine_prompt = ChatPromptTemplate.from_template(
        """
        Your job is to produce a final summary.
        We have provided an existing summary up to a certain point: {existing_summary}
        We have the opportunity to refine the existing summary (only if needed) with some more context below.
        ------------
        {context}
        ------------
        Given the new context, refine the original summary.
        If the context isn't useful, RETURN the original summary.
        """
    )

    refine_chain = refine_prompt | llm | StrOutputParser()

    for i, doc in enumerate(docs[1:]):
        refined_summary = refine_chain.invoke(
            {"existing_summary": summary, "context": doc.page_content}
        )
        summary = refined_summary

    return summary, len(docs) - 1


@st.cache_resource()
def embed_file(file_path):

    cache_dir = LocalFileStore(f"./.cache/embeddings/{os.path.basename(file_path)}")
    loader = TextLoader(file_path)
    splitter = create_text_splitter(chunk_size=800, chunk_overlap=100)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever


st.set_page_config(
    page_title="AudioGPT",
    page_icon="ğŸ’¬",
)

st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”!

AudioGPTëŠ” ì—…ë¡œë“œí•œ ì˜ìƒ íŒŒì¼ í˜¹ì€ ìœ íŠœë¸Œ ë§í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!

ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ í˜¹ì€ ë§í¬ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”!
"""
)

with st.sidebar:

    choice = st.selectbox(
        "ì‚¬ìš©í•  í•­ëª©ì„ ê³¨ë¼ì£¼ì„¸ìš”.",
        ("File", "Youtube"),
    )

    audio_path = None

    if choice == "File":
        video = st.file_uploader(
            "Video",
            type=["mp4", "avi", "mkv", "mov"],
        )

        if video:
            with st.spinner("íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                video_content = video.read()
                video_path = f"./.cache/{video.name}"
                with open(video_path, "wb") as f:
                    f.write(video_content)
                extract_audio(video_path)
                audio_path = video_path.replace("mp4", "mp3")

    elif choice == "Youtube":
        youtube_url = st.text_input("ìœ íŠœë¸Œ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

        if youtube_url:
            if st.button("ì˜¤ë””ì˜¤ ì¶”ì¶œ"):
                with st.spinner("ìœ íŠœë¸Œì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œ ì¤‘ì…ë‹ˆë‹¤..."):
                    output_path = "./.cache/youtube_audio"
                    download_audio_from_youtube(youtube_url, output_path)
                    audio_path = f"{output_path}.mp3"
                    st.success("ì˜¤ë””ì˜¤ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    if audio_path and os.path.exists(audio_path):
        st.audio(audio_path)

        chunks_folder = "./.cache/chunks"
        if audio_path.endswith(".mp3"):
            transcript_path = audio_path.replace(".mp3", ".txt")
        else:
            transcript_path = f"{audio_path}.txt"

        with st.spinner("ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„í• ì¤‘ì…ë‹ˆë‹¤..."):
            cut_audio_in_chunks(audio_path, 10, chunks_folder)

        transcript_start = st.button("ëŒ€ë³¸ì¶”ì¶œ")
        if transcript_start:
            with st.spinner("í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤..."):
                transcript_chunks(chunks_folder, transcript_path)

if audio_path and os.path.exists(audio_path):
    transcript_tab, summary_tab, qa_tab = st.tabs(["ëŒ€ë³¸", "ìš”ì•½", "ì§ˆë¬¸"])

    with transcript_tab:
        if os.path.exists(transcript_path):
            with open(transcript_path, "r") as file:
                st.write(file.read())
        else:
            st.warning(
                "ëŒ€ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'ëŒ€ë³¸ì¶”ì¶œ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëŒ€ë³¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
            )

    with summary_tab:
        summary_start = st.button("ìš”ì•½í•˜ê¸°")

        if summary_start or st.session_state["is_summary"]:
            if os.path.exists(transcript_path):
                progress_bar = st.progress(0)
                status_text = st.empty()

                with st.status("ìš”ì•½ì¤‘ì…ë‹ˆë‹¤...") as status:
                    status.update(label="ìš”ì•½ ì¤€ë¹„ì¤‘...")
                    summary, total_docs = summary_transcript(transcript_path)

                    progress_bar.progress(100, text=f"ì´ {total_docs}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")

                    status.update(label=f"ìš”ì•½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", state="complete")

                st.write(summary)
                st.session_state["is_summary"] = True
            else:
                st.warning(
                    "ëŒ€ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'ëŒ€ë³¸ì¶”ì¶œ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëŒ€ë³¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
                )

    with qa_tab:
        if os.path.exists(transcript_path):
            retriever = embed_file(transcript_path)
            query = st.chat_input("ì˜ìƒì—ì„œ ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”.")

            load_chat_history()

            chat_prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        """You are an AI assistant that answers questions about video content.
                        Use both the information provided in the video transcript and our previous conversation history to answer the user's questions accurately.
                        
                        Follow these rules:
                        1. Prioritize information from the provided video transcript.
                        2. Remember and reference our previous conversation to provide contextually relevant answers.
                        3. For questions not related to the video content, respond as in a normal conversation.
                        4. Provide concise and clear answers.
                        5. Explain in easy-to-understand language.
                        6. Must answer in Korean.
                        
                        Here is the relevant content from the video transcript:
                        {context}
                        
                        Previous conversation history:
                        {history}
                        """,
                    ),
                    ("human", "{question}"),
                ]
            )
            if query:
                send_message(query, "human")
                chain = (
                    {
                        "context": retriever,
                        "question": RunnablePassthrough(),
                        "history": RunnableLambda(
                            st.session_state.memory.load_memory_variables
                        )
                        | itemgetter("history"),
                    }
                    | chat_prompt
                    | chat_llm
                )
                with st.chat_message("ai"):
                    result = chain.invoke(query)
                    save_memory(query, result.content)
        else:
            st.warning(
                "ëŒ€ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'ëŒ€ë³¸ì¶”ì¶œ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëŒ€ë³¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
            )
else:
    st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ìœ íŠœë¸Œ ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    st.session_state["messages"] = []
    st.session_state["memory"] = ConversationBufferMemory(
        llm=llm, max_token_limit=1000, return_messages=True
    )
    st.session_state["is_summary"] = False
