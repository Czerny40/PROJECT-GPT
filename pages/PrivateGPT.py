from langchain_community.chat_models import ChatOllama
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import CacheBackedEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st


# AI ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
class ChatCallbackHandler(BaseCallbackHandler):

    message = ""

    def on_llm_start(self, *args, **kwargs):
        # AI ì‘ë‹µ ì‹œì‘ ì‹œ ë¹ˆ ë©”ì‹œì§€ ë°•ìŠ¤ ìƒì„±
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        # AI ì‘ë‹µ ì™„ë£Œ ì‹œ ë©”ì‹œì§€ ì €ì¥
        save_message(self.message, "ai")

    def on_llm_new_token(self, token: str, *args, **kwargs):
        # ì‘ë‹µì´ í† í°ë¡œ ìƒì„±ë  ë•Œë§ˆë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œ
        self.message += token
        self.message_box.markdown(self.message)


llm = ChatOllama(
    model="deepseek-r1:7b",
    temperature=0.2,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)

# ë©”ì‹œì§€ ì €ì¥ì†Œ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# íŒŒì¼ ì„ë² ë”© í•¨ìˆ˜
@st.cache_resource(show_spinner="íŒŒì¼ì„ ë¶„ì„í•˜ê³ ìˆì–´ìš”...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/private_files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/private_embeddings/{file.name}")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n"],
        length_function=len,
    )

    file_loader = UnstructuredFileLoader(file_path)
    docs = file_loader.load_and_split(text_splitter=splitter)

    embeddings = OllamaEmbeddings(
        model="deepseek-r1:7b"
    )

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    vectorstore = FAISS.from_documents(docs, cached_embeddings)

    vector_retriever = vectorstore.as_retriever(
        search_type="mmr", # MMR(Maximum Marginal Relevance) : ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆê³¼ ë‹¤ì–‘ì„±ì„ ë™ì‹œì— ê³ ë ¤
        search_kwargs={
            "k": 4,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            "fetch_k": 20,  # í›„ë³´ í’€ í¬ê¸°
            "lambda_mult": 0.7,  # ë‹¤ì–‘ì„± vs ê´€ë ¨ì„± ê°€ì¤‘ì¹˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ì„± ì¤‘ì‹œ)
        },
    )

    # BM25 : ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ê°•ë ¥í•œ ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë¬¸ì„œì™€ ê²€ìƒ‰ì–´ ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€
    bm25_retriever = BM25Retriever.from_documents(docs)
    bm25_retriever.k = 4  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜

    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ìƒì„±
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, vector_retriever],
        weights=[0.5, 0.5],  # ê° ê²€ìƒ‰ê¸°ì˜ ê°€ì¤‘ì¹˜
    )

    return ensemble_retriever

# ë©”ì‹œì§€ í‘œì‹œ í•¨ìˆ˜
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

# ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

# ì±„íŒ… ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def load_chat_history():
    for message in st.session_state["messages"]:
        send_message(message["message"], message["role"], save=False)


def format_documents(docs):
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_template(
    """Answer the question using ONLY the following context and not your training data. If you don't know the answer just say you don't know. DON'T make anything up.
    
    Context: {context}
    Question:{question}
    """
)

st.set_page_config(
    page_title="PrivateGPT",
    page_icon="ğŸ“„",
)

st.title("PrivateGPT")

st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”!

PrivateGPTëŠ” ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ë¡œì»¬ LLM ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!

ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!
"""
)

st.sidebar.markdown(
    """
### ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
- PDF (.pdf)
- í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)
- Word ë¬¸ì„œ (.docx)

"""
)

with st.sidebar:
    file = st.file_uploader(
        ".txt, .pdf, .docx íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type=["txt", "pdf", "docx"]
    )

if file:

    retriever = embed_file(file)

    send_message("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!", "ai", save=False)
    load_chat_history()
    message = st.chat_input("ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")

    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_documents),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)

else:
    st.session_state["messages"] = []
