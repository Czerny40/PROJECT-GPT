from langchain_community.chat_models import ChatOllama
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.prompts import ChatPromptTemplate
import streamlit as st

from utils.chat_utils import initialize_messages, send_message, load_chat_history
from utils.callback_utils import ChatCallbackHandler
from utils.document_utils import (
    format_documents,
    create_text_splitter,
    save_uploaded_file,
    create_cache_dir,
)
from utils.retriever_utils import create_ensemble_retriever

prompt = ChatPromptTemplate.from_template(
    """Answer the question using ONLY the following context and not your training data. If you don't know the answer just say you don't know. DON'T make anything up.
    
    Context: {context}
    Question:{question}
    """
)

llm = ChatOllama(
    model="jinbora/deepseek-r1-Bllossom:8b",
    temperature=0.2,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)

initialize_messages()


@st.cache_resource(show_spinner="íŒŒì¼ì„ ë¶„ì„í•˜ê³ ìˆì–´ìš”...")
def embed_file(file):
    file_path = save_uploaded_file(file, "private_files")
    cache_dir = create_cache_dir(file.name, "private_embeddings")

    splitter = create_text_splitter()
    file_loader = UnstructuredFileLoader(file_path)
    docs = file_loader.load_and_split(text_splitter=splitter)

    embeddings = OllamaEmbeddings(model="jinbora/deepseek-r1-Bllossom:8b")

    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    vector_retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 4,
            "fetch_k": 20,
            "lambda_mult": 0.7,
        },
    )

    return create_ensemble_retriever(vector_retriever, docs)


st.set_page_config(
    page_title="PrivateGPT",
    page_icon="ğŸ“„",
)

st.title("PrivateGPT")

st.markdown(
    """
ì•ˆë…•í•˜ì„¸ìš”!

PrivateGPTëŠ” ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ë¡œì»¬ LLM ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ë“œë¦¬ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤!

ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!
"""
)

st.sidebar.markdown(
    """
### ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
- PDF (.pdf)
- í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)
- Word ë¬¸ì„œ (.docx)

"""
)

with st.sidebar:
    file = st.file_uploader(
        ".txt, .pdf, .docx íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type=["txt", "pdf", "docx"]
    )

if file:
    retriever = embed_file(file)
    send_message("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!", "ai", save=False)
    load_chat_history()
    message = st.chat_input("ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”")

    if message:
        send_message(message, "human")

        chain = (
            {
                "context": retriever | RunnableLambda(format_documents),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)
else:
    st.session_state["messages"] = []
